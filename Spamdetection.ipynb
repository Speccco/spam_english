{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yCAy1yxMmhgv"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GRU, LSTM, Bidirectional\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-OKMdPRnQW7",
    "outputId": "f3d277f2-0788-40fc-de4d-cadea6d9d534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       is_sarcastic                                           headline\n",
      "0                 1  @USER @USER @USER I don't get this .. obviousl...\n",
      "1                 1  @USER @USER trying to protest about . Talking ...\n",
      "2                 1  @USER @USER @USER He makes an insane about of ...\n",
      "3                 1  @USER @USER Meanwhile Trump won't even release...\n",
      "4                 1  @USER @USER Pretty Sure the Anti-Lincoln Crowd...\n",
      "...             ...                                                ...\n",
      "28614             1       jews to celebrate rosh hashasha or something\n",
      "28615             1  internal affairs investigator disappointed con...\n",
      "28616             0  the most beautiful acceptance speech this week...\n",
      "28617             1  mars probe destroyed by orbiting spielberg-gat...\n",
      "28618             1                 dad clarifies this not a food stop\n",
      "\n",
      "[33619 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data_1 = pd.read_json(\"Dataset\\\\Spam_Detection.json\", lines=True)\n",
    "del data_1[\"context\"]\n",
    "data_2 = pd.read_json(\"Dataset\\\\Spam_Headlines_Dataset.json\", lines=True)\n",
    "del data_2[\"article_link\"]\n",
    "data =  pd.concat([data_1,data_2])\n",
    "data.head()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSnTgkT8CcN_"
   },
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "W9Vloo1LqP8L"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = pattern.sub('', text)\n",
    "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))           #remove @\n",
    "    emoji = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001FFFF\"                     # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"                     # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"                     # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"                     # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = emoji.sub(r'', text)\n",
    "    text = text.lower()\n",
    "    # Limitation and stamming \n",
    "    # convert \" ' \" to actual word\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)        \n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text) \n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"did't\", \"did not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"have't\", \"have not\", text)\n",
    "    text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_L9EZAKiqURU",
    "outputId": "57fa4db5-d3f7-43bc-c10f-327179e40209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize     #analyse and tekonize texts\n",
    "from nltk.corpus import stopwords           #remove  some words without value 'the,and,of'\n",
    "\n",
    "def CleanTokenize(df):\n",
    "    head_lines = list()\n",
    "    lines = df[\"headline\"].values.tolist()       #lines = data[\"headline\"]     #a=pd.array(lines, dtype=\"string\")\n",
    "    for line in lines:\n",
    "        line = clean_text(line)\n",
    "        tokens = word_tokenize(line)                            # tokenize the text\n",
    "        table = str.maketrans('', '', string.punctuation)       # remove puntuations\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]   # remove non alphabetic characters\n",
    "        stop_words = set(stopwords.words(\"english\"))            # remove stop words\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        head_lines.append(words)\n",
    "    return head_lines\n",
    "\n",
    "head_lines = CleanTokenize(data) #store the cleaned and tokenized headlines in the head_lines variable\n",
    "head_lines[0:1]\n",
    "print(type(head_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2AxrLSTqfmZ",
    "outputId": "51f00087-6233-410b-866c-238e38096a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tokens -  33331\n",
      "vocab size - 33332\n"
     ]
    }
   ],
   "source": [
    "validation_split = 0.2  #sets the fraction of the data that will be used for validation\n",
    "max_length = 25         #sets the maximum length of the input sequences\n",
    "tokenizer_obj = Tokenizer() #initializes a Tokenizer object.\n",
    "tokenizer_obj.fit_on_texts(head_lines) # fits the Tokenizer object on the tokenized and cleaned text data. \n",
    "sequences = tokenizer_obj.texts_to_sequences(head_lines) #converts the tokenized sequences to numerical sequences using the fitted Tokenizer object.\n",
    "\n",
    "word_index = tokenizer_obj.word_index  #The word_index dictionary contains key-value pairs where each word in the text data is a key and its corresponding integer index is the value.\n",
    "print(\"unique tokens - \",len(word_index)) #The len(word_index) function returns the number of unique words in the text data.\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1 #The vocab_size variable is initialized to the number of unique words in the text data plus one. This is because the integer index starts from 1 and not 0. The vocab_size indicates the size of the vocabulary that will be used to train the machine learning model.\n",
    "print('vocab size -', vocab_size)\n",
    "\n",
    "lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')                                          #pad_sequences function is used to pad the sequences to a fixed length of max_length. The sequences variable contains the tokenized sequences for each text headline. padding='post' means that the padding will be added to the end of each sequence.\n",
    "sentiment =  data['is_sarcastic'].values #extracts the labels for each text from the data object and assigns them to the sentiment variable. This creates a NumPy array containing the sentiment labels for all the headlines in the dataset.\n",
    "\n",
    "indices = np.arange(lines_pad.shape[0]) #creates an array of indices from 0 to the number of rows in the lines_pad array\n",
    "np.random.shuffle(indices) #shuffles the indices randomly\n",
    "lines_pad = lines_pad[indices] #use the shuffled indices to rearrange the order of the rows in the lines_pad and sentiment arrays, so that the rows are in a different order than they were before.\n",
    "sentiment = sentiment[indices]\n",
    "\n",
    "num_validation_samples = int(validation_split * lines_pad.shape[0])#                                                         stores the number of validation samples, which is calculated by multiplying the validation split value with the total number of padded sequences.\n",
    "\n",
    "X_train = lines_pad[:-num_validation_samples]    \n",
    "y_train = sentiment[:-num_validation_samples]\n",
    "X_test = lines_pad[-num_validation_samples:]\n",
    "y_test = sentiment[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHZ7iOmLCs62",
    "outputId": "f6a39ed9-3efe-443d-d3e5-e6f5cb8d96ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (26896, 25)\n",
      "Shape of y_train: (26896,)\n",
      "Shape of X_test: (6723, 25)\n",
      "Shape of y_test: (6723,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train:' , X_train.shape)\n",
    "print('Shape of y_train:' , y_train.shape)\n",
    "print('Shape of X_test:'  , X_test.shape)\n",
    "print('Shape of y_test:'  , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BAFTPrJzC6fc",
    "outputId": "bd54af88-fc51-40fb-b365-ed4438cc20dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4907 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "embedding_dim = 100\n",
    "GLOVE_DIR = \"Dataset\\\\GLOVE.txt\"\n",
    "f = open(GLOVE_DIR, encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()                                     #splitting each line into a list of values.\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')           #convert the text after word to array\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))       #prints the number of word vectors that were found in the file.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEePpsTaWPCw",
    "outputId": "ae24e78e-f873-49a7-dc5e-51b5f51e3ab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2292\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "c = 0\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        c+=1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(c)\n",
    "#The variable c keeps track of words Number for which the embedding vectors are found in the embeddings_index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ftref7mfWXKY"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            embedding_dim,                #the size of the embedding vector.\n",
    "                            weights=[embedding_matrix],   #the pre-trained embedding matrix.\n",
    "                            input_length=max_length,      #the length of the input sequences.\n",
    "                            trainable=False)  #the weights of the embedding layer are frozen and wont be updated during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKd-FDmRWa6H",
    "outputId": "94e03805-3c2e-45ba-fc1a-8bf7ab1d64ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Summary of the built model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 100)           3333200   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,375,505\n",
      "Trainable params: 42,305\n",
      "Non-trainable params: 3,333,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zVdV1VIWes9",
    "outputId": "1dcc8f68-6041-4fef-e35a-2b5d7707c922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "841/841 - 2101s - loss: 0.6400 - acc: 0.6295 - val_loss: 0.6116 - val_acc: 0.6621\n",
      "Epoch 2/25\n",
      "841/841 - 1897s - loss: 0.5994 - acc: 0.6723 - val_loss: 0.5896 - val_acc: 0.6748\n",
      "Epoch 3/25\n",
      "841/841 - 590s - loss: 0.5816 - acc: 0.6842 - val_loss: 0.5784 - val_acc: 0.6866\n",
      "Epoch 4/25\n",
      "841/841 - 578s - loss: 0.5693 - acc: 0.6916 - val_loss: 0.5775 - val_acc: 0.6906\n",
      "Epoch 5/25\n",
      "841/841 - 675s - loss: 0.5576 - acc: 0.6987 - val_loss: 0.5779 - val_acc: 0.6867\n",
      "Epoch 6/25\n",
      "841/841 - 847s - loss: 0.5482 - acc: 0.7090 - val_loss: 0.5826 - val_acc: 0.6933\n",
      "Epoch 7/25\n",
      "841/841 - 640s - loss: 0.5408 - acc: 0.7151 - val_loss: 0.5625 - val_acc: 0.6985\n",
      "Epoch 8/25\n",
      "841/841 - 565s - loss: 0.5304 - acc: 0.7216 - val_loss: 0.5681 - val_acc: 0.6918\n",
      "Epoch 9/25\n",
      "841/841 - 565s - loss: 0.5276 - acc: 0.7248 - val_loss: 0.5800 - val_acc: 0.6955\n",
      "Epoch 10/25\n",
      "841/841 - 572s - loss: 0.5188 - acc: 0.7316 - val_loss: 0.5566 - val_acc: 0.7004\n",
      "Epoch 11/25\n",
      "841/841 - 555s - loss: 0.5109 - acc: 0.7351 - val_loss: 0.5639 - val_acc: 0.7047\n",
      "Epoch 12/25\n",
      "841/841 - 583s - loss: 0.5053 - acc: 0.7385 - val_loss: 0.5649 - val_acc: 0.7010\n",
      "Epoch 13/25\n",
      "841/841 - 579s - loss: 0.4973 - acc: 0.7441 - val_loss: 0.5813 - val_acc: 0.6960\n",
      "Epoch 14/25\n",
      "841/841 - 618s - loss: 0.4945 - acc: 0.7448 - val_loss: 0.5704 - val_acc: 0.7036\n",
      "Epoch 15/25\n",
      "841/841 - 589s - loss: 0.4876 - acc: 0.7534 - val_loss: 0.5680 - val_acc: 0.7028\n",
      "Epoch 16/25\n",
      "841/841 - 591s - loss: 0.4828 - acc: 0.7528 - val_loss: 0.5753 - val_acc: 0.7034\n",
      "Epoch 17/25\n",
      "841/841 - 633s - loss: 0.4800 - acc: 0.7547 - val_loss: 0.5661 - val_acc: 0.7046\n",
      "Epoch 18/25\n",
      "841/841 - 618s - loss: 0.4730 - acc: 0.7594 - val_loss: 0.6022 - val_acc: 0.7071\n",
      "Epoch 19/25\n",
      "841/841 - 555s - loss: 0.4722 - acc: 0.7629 - val_loss: 0.5947 - val_acc: 0.7100\n",
      "Epoch 20/25\n",
      "841/841 - 592s - loss: 0.4637 - acc: 0.7667 - val_loss: 0.5825 - val_acc: 0.6988\n",
      "Epoch 21/25\n",
      "841/841 - 587s - loss: 0.4604 - acc: 0.7686 - val_loss: 0.5920 - val_acc: 0.7094\n",
      "Epoch 22/25\n",
      "841/841 - 575s - loss: 0.4555 - acc: 0.7726 - val_loss: 0.5845 - val_acc: 0.7083\n",
      "Epoch 23/25\n",
      "841/841 - 573s - loss: 0.4519 - acc: 0.7727 - val_loss: 0.5962 - val_acc: 0.7114\n",
      "Epoch 24/25\n",
      "841/841 - 548s - loss: 0.4486 - acc: 0.7731 - val_loss: 0.6085 - val_acc: 0.7091\n",
      "Epoch 25/25\n",
      "841/841 - 537s - loss: 0.4481 - acc: 0.7725 - val_loss: 0.6332 - val_acc: 0.7030\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=25, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "C0uecE9adGm-"
   },
   "outputs": [],
   "source": [
    "def predict_sarcasm(s):\n",
    "    x_final = pd.DataFrame({\"headline\":[s]})\n",
    "    test_lines = CleanTokenize(x_final)\n",
    "    test_sequences = tokenizer_obj.texts_to_sequences(test_lines)\n",
    "    test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "    pred = model.predict(test_review_pad)\n",
    "    pred*=100\n",
    "    if pred[0][0]>=50: return \"Spam!\"\n",
    "    else: return \"Not Spam.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "def apply_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "def apply_lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Applying stemming and lemmatization to the headlines\n",
    "stemmed_headlines = [apply_stemming(\" \".join(tokens)) for tokens in head_lines]\n",
    "lemmatized_headlines = [apply_lemmatization(\" \".join(tokens)) for tokens in head_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Applying Bag of Words (BOW)\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_bow = bow_vectorizer.fit_transform(lemmatized_headlines)\n",
    "\n",
    "# Applying TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(lemmatized_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Clustering - Accuracy: 0.5142\n",
      "Decision Tree - Accuracy: 0.5770\n",
      "Random Forest - Accuracy: 0.6220\n",
      "Naive Bayes - Accuracy: 0.5521\n",
      "Support Vector Machine - Accuracy: 0.5691\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Applying K-Means Clustering (k=2)\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X_train)\n",
    "kmeans_labels = kmeans.predict(X_test)\n",
    "accuracy_kmeans = accuracy_score(y_test, kmeans_labels)\n",
    "print(f\"K-Means Clustering - Accuracy: {accuracy_kmeans:.4f}\")\n",
    "\n",
    "# Applying Decision Tree Classifier\n",
    "model2 = DecisionTreeClassifier(random_state=42)\n",
    "model2.fit(X_train, y_train)\n",
    "y_pred_dt = model2.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"Decision Tree - Accuracy: {accuracy_dt:.4f}\")\n",
    "\n",
    "# Applying Random Forest Classifier\n",
    "model3 = RandomForestClassifier(random_state=42)\n",
    "model3.fit(X_train, y_train)\n",
    "y_pred_rf = model3.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest - Accuracy: {accuracy_rf:.4f}\")\n",
    "\n",
    "# Applying Naive Bayes Classifier\n",
    "model4 = MultinomialNB()\n",
    "model4.fit(X_train, y_train)\n",
    "y_pred_nb = model4.predict(X_test)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f\"Naive Bayes - Accuracy: {accuracy_nb:.4f}\")\n",
    "\n",
    "# Applying Support Vector Machine (SVM)\n",
    "model5 = SVC()\n",
    "model5.fit(X_train, y_train)\n",
    "y_pred_svm = model5.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"Support Vector Machine - Accuracy: {accuracy_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/23\n",
      "841/841 - 542s - loss: 0.4416 - acc: 0.7756 - val_loss: 0.6001 - val_acc: 0.7050\n",
      "Epoch 2/23\n",
      "841/841 - 540s - loss: 0.4434 - acc: 0.7743 - val_loss: 0.6016 - val_acc: 0.7071\n",
      "Epoch 3/23\n",
      "841/841 - 541s - loss: 0.4374 - acc: 0.7794 - val_loss: 0.5990 - val_acc: 0.7083\n",
      "Epoch 4/23\n",
      "841/841 - 627s - loss: 0.4341 - acc: 0.7848 - val_loss: 0.6173 - val_acc: 0.7107\n",
      "Epoch 5/23\n",
      "841/841 - 33976s - loss: 0.4310 - acc: 0.7829 - val_loss: 0.6098 - val_acc: 0.7086\n",
      "Epoch 6/23\n",
      "841/841 - 527s - loss: 0.4270 - acc: 0.7849 - val_loss: 0.6190 - val_acc: 0.7104\n",
      "Epoch 7/23\n",
      "841/841 - 553s - loss: 0.4236 - acc: 0.7901 - val_loss: 0.6175 - val_acc: 0.7120\n",
      "Epoch 8/23\n",
      "841/841 - 505s - loss: 0.4235 - acc: 0.7880 - val_loss: 0.6207 - val_acc: 0.7120\n",
      "Epoch 9/23\n",
      "841/841 - 482s - loss: 0.4224 - acc: 0.7906 - val_loss: 0.6286 - val_acc: 0.7095\n",
      "Epoch 10/23\n",
      "841/841 - 480s - loss: 0.4163 - acc: 0.7922 - val_loss: 0.6273 - val_acc: 0.7088\n",
      "Epoch 11/23\n",
      "841/841 - 468s - loss: 0.4162 - acc: 0.7916 - val_loss: 0.6314 - val_acc: 0.7028\n",
      "Epoch 12/23\n",
      "841/841 - 482s - loss: 0.4137 - acc: 0.7928 - val_loss: 0.6366 - val_acc: 0.7052\n",
      "Epoch 13/23\n",
      "841/841 - 480s - loss: 0.4136 - acc: 0.7957 - val_loss: 0.6242 - val_acc: 0.7071\n",
      "Epoch 14/23\n",
      "841/841 - 480s - loss: 0.4106 - acc: 0.7949 - val_loss: 0.6448 - val_acc: 0.7086\n",
      "Epoch 15/23\n",
      "841/841 - 483s - loss: 0.4089 - acc: 0.7986 - val_loss: 0.6501 - val_acc: 0.7086\n",
      "Epoch 16/23\n",
      "841/841 - 481s - loss: 0.4025 - acc: 0.8003 - val_loss: 0.6317 - val_acc: 0.7010\n",
      "Epoch 17/23\n",
      "841/841 - 481s - loss: 0.4049 - acc: 0.7990 - val_loss: 0.6336 - val_acc: 0.7049\n",
      "Epoch 18/23\n",
      "841/841 - 480s - loss: 0.4028 - acc: 0.8005 - val_loss: 0.6628 - val_acc: 0.7052\n",
      "Epoch 19/23\n",
      "841/841 - 482s - loss: 0.4011 - acc: 0.8009 - val_loss: 0.6336 - val_acc: 0.7059\n",
      "Epoch 20/23\n",
      "841/841 - 481s - loss: 0.3945 - acc: 0.8040 - val_loss: 0.6523 - val_acc: 0.6949\n",
      "Epoch 21/23\n",
      "841/841 - 481s - loss: 0.3957 - acc: 0.8056 - val_loss: 0.6506 - val_acc: 0.7034\n",
      "Epoch 22/23\n",
      "841/841 - 486s - loss: 0.3943 - acc: 0.8076 - val_loss: 0.6727 - val_acc: 0.7018\n",
      "Epoch 23/23\n",
      "841/841 - 556s - loss: 0.3909 - acc: 0.8075 - val_loss: 0.6648 - val_acc: 0.7082\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=23, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "841/841 - 489s - loss: 0.3924 - acc: 0.8059 - val_loss: 0.6444 - val_acc: 0.7021\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.70207, saving model to best_spam_model.h5\n",
      "Epoch 2/25\n",
      "841/841 - 489s - loss: 0.3897 - acc: 0.8069 - val_loss: 0.6852 - val_acc: 0.7080\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.70207 to 0.70802, saving model to best_spam_model.h5\n",
      "Epoch 3/25\n",
      "841/841 - 484s - loss: 0.3897 - acc: 0.8093 - val_loss: 0.6397 - val_acc: 0.7056\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.70802\n",
      "Epoch 4/25\n",
      "841/841 - 486s - loss: 0.3904 - acc: 0.8088 - val_loss: 0.6279 - val_acc: 0.7079\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.70802\n",
      "Epoch 5/25\n",
      "841/841 - 478s - loss: 0.3876 - acc: 0.8112 - val_loss: 0.6539 - val_acc: 0.7019\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.70802\n",
      "Epoch 6/25\n",
      "841/841 - 487s - loss: 0.3868 - acc: 0.8093 - val_loss: 0.6580 - val_acc: 0.7088\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.70802 to 0.70876, saving model to best_spam_model.h5\n",
      "Epoch 7/25\n",
      "841/841 - 493s - loss: 0.3838 - acc: 0.8094 - val_loss: 0.6860 - val_acc: 0.7059\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.70876\n",
      "Epoch 8/25\n",
      "841/841 - 546s - loss: 0.3851 - acc: 0.8124 - val_loss: 0.6415 - val_acc: 0.7068\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.70876\n",
      "Epoch 9/25\n",
      "841/841 - 530s - loss: 0.3815 - acc: 0.8126 - val_loss: 0.6422 - val_acc: 0.7053\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.70876\n",
      "Epoch 10/25\n",
      "841/841 - 516s - loss: 0.3800 - acc: 0.8118 - val_loss: 0.6843 - val_acc: 0.7046\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.70876\n",
      "Epoch 11/25\n",
      "841/841 - 569s - loss: 0.3790 - acc: 0.8155 - val_loss: 0.6816 - val_acc: 0.7059\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.70876\n",
      "Epoch 12/25\n",
      "841/841 - 553s - loss: 0.3757 - acc: 0.8172 - val_loss: 0.6446 - val_acc: 0.7052\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.70876\n",
      "Epoch 13/25\n",
      "841/841 - 533s - loss: 0.3739 - acc: 0.8151 - val_loss: 0.6626 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.70876\n",
      "Epoch 14/25\n",
      "841/841 - 556s - loss: 0.3770 - acc: 0.8149 - val_loss: 0.6530 - val_acc: 0.7073\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.70876\n",
      "Epoch 15/25\n",
      "841/841 - 568s - loss: 0.3725 - acc: 0.8192 - val_loss: 0.6542 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.70876\n",
      "Epoch 16/25\n",
      "841/841 - 708s - loss: 0.3719 - acc: 0.8173 - val_loss: 0.6724 - val_acc: 0.7047\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.70876\n",
      "Epoch 17/25\n",
      "841/841 - 669s - loss: 0.3751 - acc: 0.8162 - val_loss: 0.6733 - val_acc: 0.7047\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.70876\n",
      "Epoch 18/25\n",
      "841/841 - 674s - loss: 0.3717 - acc: 0.8187 - val_loss: 0.7046 - val_acc: 0.7019\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.70876\n",
      "Epoch 19/25\n",
      "841/841 - 524s - loss: 0.3683 - acc: 0.8211 - val_loss: 0.6936 - val_acc: 0.7049\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.70876\n",
      "Epoch 20/25\n",
      "841/841 - 506s - loss: 0.3713 - acc: 0.8202 - val_loss: 0.6885 - val_acc: 0.7033\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.70876\n",
      "Epoch 21/25\n",
      "841/841 - 500s - loss: 0.3680 - acc: 0.8197 - val_loss: 0.6968 - val_acc: 0.7022\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.70876\n",
      "Epoch 22/25\n",
      "841/841 - 501s - loss: 0.3688 - acc: 0.8197 - val_loss: 0.6954 - val_acc: 0.7068\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.70876\n",
      "Epoch 23/25\n",
      "841/841 - 501s - loss: 0.3664 - acc: 0.8212 - val_loss: 0.6563 - val_acc: 0.7076\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.70876\n",
      "Epoch 24/25\n",
      "841/841 - 501s - loss: 0.3657 - acc: 0.8214 - val_loss: 0.7100 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.70876\n",
      "Epoch 25/25\n",
      "841/841 - 529s - loss: 0.3606 - acc: 0.8253 - val_loss: 0.6729 - val_acc: 0.7044\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.70876\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint callback\n",
    "checkpoint_path = \"best_spam_model.h5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_acc', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "# Train the model with checkpointing\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=32, \n",
    "                    epochs=25, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=[checkpoint], \n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "def predict_sarcasm(sentence):\n",
    "    model = load_model(\"best_spam_model.h5\")  # Load the saved model\n",
    "    x_final = pd.DataFrame({\"headline\": [sentence]})\n",
    "    \n",
    "    test_lines = CleanTokenize(x_final)  # Clean and tokenize input text\n",
    "    test_sequences = tokenizer_obj.texts_to_sequences(test_lines)\n",
    "    test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "    \n",
    "    pred = model.predict(test_review_pad)[0][0] * 100  # Get prediction score\n",
    "    return \"Spam!\" if pred >= 50 else \"Not Spam.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer saved as tokenizer.pkl!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer_obj, handle)\n",
    "\n",
    "print(\"✅ Tokenizer saved as tokenizer.pkl!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = load_model(\"best_spam_model.h5\", compile=False)\n",
    "    model.save(\"best_spam_model.keras\", save_format=\"keras\")\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "     print(\"Error loading model:\" , e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to C:\\Users\\user/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"cd huggingface_model_repo && git add . && git commit -m 'Upload trained model and tokenizer' && git push origin main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "repo_name = \"Ahmad1020/sarcasm-spam-detector\"  # Your Hugging Face repo\n",
    "local_repo_path = \"./huggingface_model_repo\"\n",
    "\n",
    "# Authenticate and create repo (if not already created)\n",
    "api = HfApi()\n",
    "api.create_repo(repo_name, exist_ok=True)\n",
    "\n",
    "# Clone the repo manually using git\n",
    "#os.system(f\"git clone https://huggingface.co/{repo_name} {local_repo_path}\")\n",
    "\n",
    "# Ensure the repo exists\n",
    "os.makedirs(local_repo_path, exist_ok=True)\n",
    "\n",
    "# Copy model and tokenizer files\n",
    "model_path = \"best_spam_model.keras\"\n",
    "tokenizer_path = \"tokenizer.pkl\"\n",
    "\n",
    "shutil.copy(model_path, f\"{local_repo_path}/best_spam_model.keras\")\n",
    "shutil.copy(tokenizer_path, f\"{local_repo_path}/tokenizer.pkl\")\n",
    "\n",
    "# Push changes using direct git commands\n",
    "os.system(f\"cd {local_repo_path} && git add . && git commit -m 'Upload trained model and tokenizer' && git push\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model loaded successfully!\n",
      "Tokenizer loaded successfully!\n",
      "Prediction: Not Spam (Confidence: 0.4694)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Hugging Face repository URL\n",
    "repo_url = \"https://huggingface.co/Ahmad1020/sarcasm-spam-detector/resolve/main/\"\n",
    "\n",
    "# Define filenames\n",
    "model_filename = \"best_spam_model.keras\"\n",
    "tokenizer_filename = \"tokenizer.pkl\"\n",
    "\n",
    "# Download model file\n",
    "if not os.path.exists(model_filename):\n",
    "    print(\"Downloading model...\")\n",
    "    model_response = requests.get(repo_url + model_filename)\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        f.write(model_response.content)\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_filename)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Download tokenizer file\n",
    "if not os.path.exists(tokenizer_filename):\n",
    "    print(\"Downloading tokenizer...\")\n",
    "    tokenizer_response = requests.get(repo_url + tokenizer_filename)\n",
    "    with open(tokenizer_filename, \"wb\") as f:\n",
    "        f.write(tokenizer_response.content)\n",
    "\n",
    "# Load the tokenizer\n",
    "with open(tokenizer_filename, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "print(\"Tokenizer loaded successfully!\")\n",
    "\n",
    "# Test the model\n",
    "sample_text = [\"This is an example spam message.\"]\n",
    "sequences = tokenizer.texts_to_sequences(sample_text)\n",
    "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=25)\n",
    "\n",
    "prediction = model.predict(padded)[0][0]\n",
    "label = \"Spam\" if prediction > 0.5 else \"Not Spam\"\n",
    "\n",
    "print(f\"Prediction: {label} (Confidence: {prediction:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "✅ Model loaded successfully!\n",
      "✅ Tokenizer loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# تعطيل الـ GPU لتجنب مشاكل CUDA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# Hugging Face repository URL\n",
    "repo_url = \"https://huggingface.co/Ahmad1020/sarcasm-spam-detector/resolve/main/\"\n",
    "\n",
    "# Define filenames\n",
    "model_filename = \"best_spam_model.keras\"\n",
    "tokenizer_filename = \"tokenizer.pkl\"\n",
    "max_length = 25  # يجب أن يكون نفس max_length المستخدم أثناء التدريب\n",
    "\n",
    "# تحميل النموذج إذا لم يكن موجودًا\n",
    "if not os.path.exists(model_filename):\n",
    "    print(f\"Downloading {model_filename}...\")\n",
    "    model_response = requests.get(repo_url + model_filename)\n",
    "    if model_response.status_code == 200:\n",
    "        with open(model_filename, \"wb\") as f:\n",
    "            f.write(model_response.content)\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download {model_filename}. Status code: {model_response.status_code}\")\n",
    "\n",
    "# تحميل التوكنيزر إذا لم يكن موجودًا\n",
    "if not os.path.exists(tokenizer_filename):\n",
    "    print(f\"Downloading {tokenizer_filename}...\")\n",
    "    tokenizer_response = requests.get(repo_url + tokenizer_filename)\n",
    "    if tokenizer_response.status_code == 200:\n",
    "        with open(tokenizer_filename, \"wb\") as f:\n",
    "            f.write(tokenizer_response.content)\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download {tokenizer_filename}. Status code: {tokenizer_response.status_code}\")\n",
    "\n",
    "# تحميل النموذج\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_filename)\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error loading model: {e}\")\n",
    "\n",
    "# تحميل التوكنيزر\n",
    "try:\n",
    "    with open(tokenizer_filename, \"rb\") as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    print(\"✅ Tokenizer loaded successfully!\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error loading tokenizer: {e}\")\n",
    "\n",
    "# دالة توقع السخرية\n",
    "def predict_sarcasm(sentence):\n",
    "    if not isinstance(sentence, str) or not sentence.strip():\n",
    "        return False  # التعامل مع الإدخال غير الصالح\n",
    "\n",
    "    test_sequences = tokenizer.texts_to_sequences([sentence])\n",
    "    test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "    pred = model.predict(test_review_pad)[0][0] * 100\n",
    "    return pred >= 50  # إرجاع True إذا كان احتمال السخرية ≥ 50%\n",
    "\n",
    "predict_sarcasm(\"what a fucken match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# تعطيل الـ GPU لتجنب مشاكل CUDA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# رابط Hugging Face\n",
    "REPO_URL = \"https://huggingface.co/Ahmad1020/sarcasm-spam-detector/resolve/main/\"\n",
    "\n",
    "# حفظ الملفات في `/tmp/`\n",
    "MODEL_PATH = \"/best_spam_model.keras\"\n",
    "TOKENIZER_PATH = \"/tokenizer.pkl\"\n",
    "MAX_LENGTH = 25  # يجب أن يكون نفس max_length المستخدم أثناء التدريب\n",
    "\n",
    "# تحميل الملفات إلى `/tmp/`\n",
    "def download_file(filename, save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        response = requests.get(REPO_URL + filename)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "        else:\n",
    "            raise Exception(f\"Failed to download {filename}. Status code: {response.status_code}\")\n",
    "\n",
    "download_file(\"best_spam_model.keras\", MODEL_PATH)\n",
    "download_file(\"tokenizer.pkl\", TOKENIZER_PATH)\n",
    "\n",
    "# تحميل النموذج\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# تحميل التوكنيزر\n",
    "with open(TOKENIZER_PATH, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# إنشاء FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# نموذج الإدخال\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "# دالة التنبؤ\n",
    "def predict_sarcasm(sentence):\n",
    "    test_sequences = tokenizer.texts_to_sequences([sentence])\n",
    "    test_review_pad = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post')\n",
    "    pred = model.predict(test_review_pad)[0][0] * 100\n",
    "    return {\"text\": sentence, \"sarcasm\": pred >= 50, \"confidence\": round(pred, 2)}\n",
    "\n",
    "# نقطة النهاية API\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: TextInput):\n",
    "    return predict_sarcasm(data.text)\n",
    "predict_sarcasm(\"what a fucken match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loading model...\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "✅ Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# تعطيل الـ GPU لتجنب مشاكل CUDA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# مسارات الملفات المخزنة محليًا\n",
    "MODEL_PATH = \"best_spam_model.keras\"\n",
    "TOKENIZER_PATH = \"tokenizer.pkl\"\n",
    "MAX_LENGTH = 25  # يجب أن يكون نفس max_length المستخدم أثناء التدريب\n",
    "\n",
    "# التحقق مما إذا كان النموذج والتوكنيزر موجودين\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Model file not found: {MODEL_PATH}\")\n",
    "if not os.path.exists(TOKENIZER_PATH):\n",
    "    raise FileNotFoundError(f\"Tokenizer file not found: {TOKENIZER_PATH}\")\n",
    "\n",
    "# تحميل النموذج\n",
    "print(\"✅ Loading model...\")\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# تحميل التوكنيزر\n",
    "print(\"✅ Loading tokenizer...\")\n",
    "with open(TOKENIZER_PATH, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# إنشاء FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# نموذج الإدخال\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "# دالة التنبؤ\n",
    "def predict_sarcasm(sentence):\n",
    "    test_sequences = tokenizer.texts_to_sequences([sentence])\n",
    "    test_review_pad = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post')\n",
    "    pred = model.predict(test_review_pad)[0][0] * 100\n",
    "    return {\"text\": sentence, \"sarcasm\": pred >= 50, \"confidence\": round(pred, 2)}\n",
    "\n",
    "# نقطة النهاية API\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: TextInput):\n",
    "    return predict_sarcasm(data.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Download the model\n",
    "model_path = hf_hub_download(repo_id=\"your-username/sarcasm-spam-detector\", filename=\"best_spam_model.keras\")\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Download the tokenizer\n",
    "tokenizer_path = hf_hub_download(repo_id=\"your-username/sarcasm-spam-detector\", filename=\"tokenizer.pkl\")\n",
    "with open(tokenizer_path, \"rb\") as handle:\n",
    "    tokenizer_obj = pickle.load(handle)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"best_spam_model.keras\"):\n",
    "     model = keras.models.load_model(\"best_spam_model.h5\", compile=False)\n",
    "# else:\n",
    "#     model = keras.models.load_model(\"best_spam_model_updated.keras\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Spam!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"what a fucken match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "VjgI86SldTHq",
    "outputId": "ebb55004-13be-4b79-eaf5-187ab9a04e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Not Spam.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"كان ماتش سهل و خسرته بغبائك\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "kwgiiMMPdXcX",
    "outputId": "e22b4611-74c9-47a2-80c9-64ff68156cd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Not Spam.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"you looks a professional player\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xxZ-4753daog",
    "outputId": "8a646be4-b9af-441c-9a6c-4d1053e285a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F034BE620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Spam!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"well done bro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MlgUYzb1ddHz",
    "outputId": "2c3c231c-3d41-4eec-a588-24df0ec87e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F034BE7B8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Not Spam.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"جامد اوي ياخويا هستنى منك الافضل\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "k-I9FqvHdiUx",
    "outputId": "3469383e-984c-4dbd-830c-f376d8cea099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Not Spam.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"عرفنا انك جامد ياعم\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dPzDi0YmdoDo",
    "outputId": "dae5e88d-5fb1-4ff8-d41b-1da1a1d04198"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Spam!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"what an idiot coach, he should give you time on the field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "D10i-eqNeiQw",
    "outputId": "6b12e81c-629e-480e-c446-67a172afb742"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not Spam.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"كان لازم المدرب يديلك وقتك في الملعب\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spam!'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"كسم الاهلي\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spam!'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"fuck elahly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not Spam.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"الزمالك عمهم\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not Spam.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"Elzamalek king of the play\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22804\\3237060757.py:2: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking library versions...\n",
      "\n",
      "fastapi == 0.116.1\n",
      "huggingface_hub == 0.29.2\n",
      "keras == 3.10.0\n",
      "matplotlib == 3.9.4\n",
      "nltk == 3.9.1\n",
      "numpy == 1.24.3\n",
      "pandas == 2.2.3\n",
      "pydantic == 2.11.7\n",
      "requests == 2.32.4\n",
      "scikit-learn == 1.6.1\n",
      "tensorflow == 2.19.0\n",
      "uvicorn == 0.34.2\n",
      "os (built-in or no version info)\n",
      "pickle (built-in or no version info)\n",
      "re (built-in or no version info)\n",
      "shutil (built-in or no version info)\n",
      "string (built-in or no version info)\n",
      "\n",
      "✅ Done! File 'requirements.txt' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pkg_resources\n",
    "\n",
    "# مكتبات المشروع\n",
    "libraries = [\n",
    "    \"fastapi\",\n",
    "    \"huggingface_hub\",\n",
    "    \"keras\",\n",
    "    \"matplotlib\",\n",
    "    \"nltk\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"pydantic\",\n",
    "    \"requests\",\n",
    "    \"scikit-learn\",\n",
    "    \"tensorflow\",\n",
    "    \"uvicorn\",\n",
    "    \"os\",\n",
    "    \"pickle\",\n",
    "    \"re\",\n",
    "    \"shutil\",\n",
    "    \"string\",\n",
    "]\n",
    "\n",
    "print(\"🔍 Checking library versions...\\n\")\n",
    "requirements = []\n",
    "\n",
    "for lib in libraries:\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(lib).version\n",
    "        print(f\"{lib} == {version}\")\n",
    "        requirements.append(f\"{lib}=={version}\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        try:\n",
    "            importlib.import_module(lib)\n",
    "            print(f\"{lib} (built-in or no version info)\")\n",
    "        except ImportError:\n",
    "            print(f\"{lib} ❌ not installed\")\n",
    "\n",
    "# إنشاء ملف requirements.txt\n",
    "with open(\"requirements.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in requirements:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"\\n✅ Done! File 'requirements.txt' created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
